{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a333582-2a54-44fb-82ef-3eeec9388e09",
   "metadata": {
    "id": "6a333582-2a54-44fb-82ef-3eeec9388e09"
   },
   "source": [
    "### Author: Helen Cai\n",
    "### April 2025\n",
    "The purpose of this notebook is to create a decoder architecture that will be used in our final project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931c3cd6-4267-41f5-8869-99aee1c01666",
   "metadata": {
    "id": "931c3cd6-4267-41f5-8869-99aee1c01666"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import tqdm\n",
    "\n",
    "# NetworkX is a Python package used to create, manipulate, and mine graphs\n",
    "import networkx as nx\n",
    "\n",
    "# further libraries for working with graphs\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, pool\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# For visualization\n",
    "import phate\n",
    "\n",
    "# Graph scattering functionality\n",
    "from LEGS_module import *\n",
    "\n",
    "# Home-grown functions\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5201e-cf66-4b5d-a53a-0db0bf9017b6",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "709a2920-cf08-492c-a18b-dfdde419acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "p = 0.2     # probability for dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4988b0-59d9-44c5-ba05-d8c14df6ff21",
   "metadata": {},
   "source": [
    "## 0. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5137727b-d560-47c5-b8ca-72fb4799359b",
   "metadata": {
    "id": "5137727b-d560-47c5-b8ca-72fb4799359b"
   },
   "outputs": [],
   "source": [
    "hidden_values = torch.load(\"hidden_values_from_insecticides.pt\", weights_only=False)\n",
    "insecticides = torch.load(\"./data/insecticides_graphs_small.pt\", weights_only=False)\n",
    "\n",
    "# note that logP values are in the y slot of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5e087-f713-4bf8-b713-e257ae145724",
   "metadata": {},
   "source": [
    "\n",
    "Because the value of D changes for each of the graphs in our dataset, we need some way to account for this. I will allow the decoding MLP to project to a size of fixed dimension, (e.g. the dimension that corresponds to the largest possible graph). Then, in my training dataset, the graphs will be padded with 0's to match the resulting dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c24d1f7-95b8-4321-ba94-a03896a52648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add padding to target matrices to match the output dimensions\n",
    "def pad_graphs(input_x, input_edge_index, input_edge_attr, D = 88):\n",
    "    extra_x = int((D/2) - input_x.shape[0])\n",
    "    padded_x = F.pad(input_x, (0, 0, 0, extra_x), value=0)\n",
    "\n",
    "    extra_edge_index = int(D - input_edge_index.shape[1])\n",
    "    padded_edge_index = F.pad(input_edge_index, (0, extra_edge_index, 0, 0), value=0)\n",
    "\n",
    "    extra_edge_attr = int(88 - input_edge_attr.shape[0])\n",
    "    padded_edge_attr = F.pad(input_edge_attr, (0, 0, 0, extra_edge_attr), value=0)\n",
    "\n",
    "    return padded_x, padded_edge_index, padded_edge_attr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760a27c4-44cd-43d7-9f08-56246cb82fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the hidden values to the graphs & add padding\n",
    "for i in range(len(insecticides)):\n",
    "    graph = insecticides[i]\n",
    "    graph.hidden_values = hidden_values[i]\n",
    "    graph.x, graph.edge_index, graph.edge_attr = pad_graphs(graph.x, graph.edge_index, graph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0ff48a-c9ae-47e9-b793-55ee0057f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders \n",
    "# split into training and test\n",
    "train_dataset, test_dataset = train_test_split(insecticides, test_size=0.2, random_state=2025)\n",
    "\n",
    "all_data = DataLoader(insecticides, batch_size=1, shuffle = False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306297dc",
   "metadata": {},
   "source": [
    "## 1. Training a latent space decoder\n",
    "\n",
    "Now that we have accomplished diffusion in the latent space, we want to be able to convert that latent space representation back to a graph representation. \n",
    "\n",
    "For our training process: our inputs are latent space points and the original graphs used to generate them. Refer to the `encoder` notebook for information on how these inputs are generated. \n",
    "\n",
    "We note that the graph data has the following (rough) dimensions: `x` $ \\in R^{D\\times3}$, `edge_index` $ \\in R^{2 \\times D}$, `edge_attr` $ \\in R^{D \\times 1}$. Thusly, these are the dimensions that our decoder will work to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981d8049-1401-44e6-9033-cfcda70c4efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the maximum number of edges we need to predict?\n",
    "max([data.edge_index.shape[1] for data in insecticides])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2166267c-ffe6-4476-b10f-83aba5765ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize an MLP for re-creating graph representations from latent space.\n",
    "    Input (x) will be latent space embeddings (tensor 16x1).\n",
    "    We need to be able to predict x, edge_index, edge_attr.\n",
    "    The number of nodes and edges will also be dynamically predicted.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_features: int = 16, # This is the size of the latent space representations.\n",
    "        D: int = 88, # TODO figure this out\n",
    "        p: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "\n",
    "        # Use an MLP to expand dimensions of latent space\n",
    "        self.mlp = nn.Sequential(\n",
    "            Linear(num_features, 32),\n",
    "            nn.ReLU(),\n",
    "            Linear(32, 64),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            Linear(64, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # one head for predicting features x\n",
    "        x_dim = int(D * 3 / 2)\n",
    "        self.x_fc = nn.Sequential(\n",
    "            Linear(128, 128),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            Linear(128, x_dim))\n",
    "\n",
    "        # one head for predicting edge_index\n",
    "        self.edge_index_fc = nn.Sequential(\n",
    "            Linear(128, 128),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            Linear(128, 2*D))\n",
    "\n",
    "        # one head for predicting edge_attr\n",
    "        self.edge_attr_fc = nn.Sequential(\n",
    "            Linear(128, 128),\n",
    "            nn.Dropout(p),\n",
    "            nn.ReLU(),\n",
    "            Linear(128, 1*D))\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        # Apply the MLP\n",
    "        h = self.mlp(data)\n",
    "        \n",
    "        # Predict x and reshape\n",
    "        x = self.x_fc(h)\n",
    "        x = x.view(-1, int(self.D / 2), 3)\n",
    "        x = torch.squeeze(x, 0)\n",
    "        \n",
    "\n",
    "        # Predict edge_index and reshape\n",
    "        edge_index = self.edge_index_fc(h)\n",
    "        edge_index = edge_index.view(-1, 2, self.D)\n",
    "        edge_index = torch.squeeze(edge_index, 0)\n",
    "        edge_index = edge_index.to(torch.int64)\n",
    "\n",
    "        # Predict edge_attr and reshape\n",
    "        edge_attr = self.edge_attr_fc(h)\n",
    "        edge_attr = edge_attr.view(-1, self.D, 1)\n",
    "        edge_attr = torch.squeeze(edge_attr, 0)\n",
    "\n",
    "        return x, edge_index, edge_attr "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45fb390-8e22-40ca-8dbf-b4c42e21600f",
   "metadata": {},
   "source": [
    "We should be thoughtful about how we calculate loss for each of these 3 different things. \n",
    "\n",
    "* for x (node features): these are floats, so we can compare them using MSE loss\n",
    "* for edge_index: these are categorical, so we should use a categorical-type loss such as  cross entropy\n",
    "* for edge_attr: these are also floats, so we can use MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004ed643-53be-4992-94ed-5345b79188bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder_epoch(model, optimizer, train_loader):\n",
    "    \"\"\"Train the model for one epoch.\n",
    "    Args:\n",
    "        model: the model\n",
    "        optimizer: the optimizer\n",
    "        train_loader: contains all information needed for training, including graphs and their latent space representations.\n",
    "    Returns:\n",
    "        train_loss: the loss of the epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_epoch = 0\n",
    "\n",
    "    # what loss functions are used for each of our three targets?\n",
    "    x_criterion = nn.MSELoss()\n",
    "    edge_index_criterion = nn.MSELoss()  # TODO: think about cross entropy loss here?\n",
    "    edge_attr_criterion = nn.MSELoss()\n",
    "    \n",
    "    # evaluate on the train nodes\n",
    "    for data in train_loader:\n",
    "        target_x = data.x\n",
    "        target_edge_index = data.edge_index\n",
    "        target_edge_attr = data.edge_attr\n",
    "        \n",
    "        batch_size = data.num_graphs\n",
    "\n",
    "        # get the outputs\n",
    "        x, edge_index, edge_attr  = model(data.hidden_values)\n",
    "\n",
    "        # calculate loss for each of the three outputs\n",
    "        x_loss = x_criterion(x, target_x)\n",
    "        edge_index_loss = edge_index_criterion(edge_index.to(torch.float), target_edge_index.to(torch.float))  ## something strange w data types here\n",
    "        edge_attr_loss = edge_attr_criterion(edge_attr, target_edge_attr)\n",
    "        \n",
    "        # aggregate the total loss\n",
    "        loss = x_loss + edge_index_loss + edge_attr_loss\n",
    "\n",
    "        loss.backward()\n",
    "        loss_epoch += loss.detach().numpy() * batch_size\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate training loss for the epoch\n",
    "    loss_epoch = loss_epoch / len(train_loader.dataset)\n",
    "\n",
    "    return loss_epoch \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "216f6c69-bc08-43bb-be3a-b4b743c534f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decoder_epoch(model, train_loader):\n",
    "    \"\"\"Test the model for one epoch.\n",
    "    Args:\n",
    "        model: the model\n",
    "        train_loader: contains all information needed for training, including graphs and their latent space representations.\n",
    "    Returns:\n",
    "        train_loss: the loss of the epoch\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode\n",
    "\n",
    "    loss_epoch = 0\n",
    "    \n",
    "    # what loss functions are used for each of our three targets?\n",
    "    x_criterion = nn.MSELoss()\n",
    "    edge_index_criterion = nn.MSELoss()  # TODO: think about cross entropy loss here?\n",
    "    edge_attr_criterion = nn.MSELoss()\n",
    "    \n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "\n",
    "        for data in train_loader:\n",
    "            target_x = data.x\n",
    "            target_edge_index = data.edge_index\n",
    "            target_edge_attr = data.edge_attr\n",
    "            \n",
    "            batch_size = data.num_graphs\n",
    "    \n",
    "            # get the outputs\n",
    "            x, edge_index, edge_attr  = model(data.hidden_values)\n",
    "    \n",
    "            # calculate loss for each of the three outputs\n",
    "            x_loss = x_criterion(x, target_x)\n",
    "            edge_index_loss = edge_index_criterion(edge_index.to(torch.float), target_edge_index.to(torch.float))  ## something strange w data types here\n",
    "            edge_attr_loss = edge_attr_criterion(edge_attr, target_edge_attr)\n",
    "            \n",
    "            # aggregate the total loss\n",
    "            loss = x_loss + edge_index_loss + edge_attr_loss\n",
    "            \n",
    "            loss_epoch += loss.detach().numpy() * batch_size\n",
    "\n",
    "        # calculate test loss for the epoch\n",
    "        loss_epoch = loss_epoch / len(test_loader.dataset)\n",
    "                \n",
    "    return loss_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67caddb5-4258-400b-9db4-bc579be70a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(model, train_loader, test_loader, optimizer, epochs=5):\n",
    "    \"\"\"Train the model.\n",
    "    Args:\n",
    "        model: the model\n",
    "        loss_fn: the loss function\n",
    "        train_loader: the training data loader\n",
    "        test_loader: the testing data loader\n",
    "        optimizer: the optimizer\n",
    "        epochs: the number of epochs to train\n",
    "    Returns:\n",
    "        train_losses: the training losses\n",
    "        test_losses: the testing losses\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    \n",
    "    test_losses = []\n",
    "    \n",
    "    loop = tqdm.tqdm(range(1, epochs + 1))\n",
    "\n",
    "    for epoch in loop:\n",
    "\n",
    "        # train the model for one epoch\n",
    "        train_loss_epoch = train_decoder_epoch(model, optimizer, train_loader)\n",
    "        \n",
    "        # test the model for one epoch        \n",
    "        test_loss_epoch = test_decoder_epoch(model, train_loader)\n",
    "\n",
    "        # put into our storage vectors\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        test_losses.append(test_loss_epoch)\n",
    "        \n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(train_loss=train_loss_epoch, test_loss=test_loss_epoch)\n",
    "    \n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc1d02-3fd5-41f5-8626-5ec5d756c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5:   5%|▌         | 5/100 [01:29<25:21, 16.01s/it, test_loss=353, train_loss=88.7]"
     ]
    }
   ],
   "source": [
    "decoder = DecodeNet(p = p) \n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.RMSprop(decoder.parameters(), lr = learning_rate)\n",
    "\n",
    "\n",
    "# Call the functions\n",
    "decoder_train_losses, decoder_test_losses  = train_decoder(decoder, train_loader, test_loader, optimizer, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ad2c6-4265-4533-be2b-f3d5516e926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(decoder_train_losses, decoder_test_losses, \n",
    "             xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Decoder training loss\", \n",
    "             fname=\"Loss.png\", subdir=\"./training-figs/decoder/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5913162c-6182-40e2-b9b7-4d06f9e80a14",
   "metadata": {},
   "source": [
    "## 2. Generation: Going from latent space into graph space\n",
    "\n",
    "Call the latent space decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336fd51-48dc-4845-9216-b8f34797b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(decoder.state_dict(), \"decoder-trained.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7fa19-7ea0-4200-b5be-bd7178ca594d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecodeNet()\n",
    "decoder.load_state_dict(torch.load(\"decoder-trained.pth\"))\n",
    "\n",
    "generated_x, generated_edge_index, generated_edge_attr = decoder(insecticides[2].hidden_values)  # TODO: replace this with diffused hidden values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7d5c5-6f06-4357-a69b-bc8bfb5c68c4",
   "metadata": {},
   "source": [
    "## 3. Export the graphs for evaluation \n",
    "\n",
    "This is where we pass things back to Tobias to work on for evaluation. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
